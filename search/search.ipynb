{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26e9ec12-815a-4073-8891-cb6888c41cdb",
   "metadata": {},
   "source": [
    "# Search\n",
    "\n",
    "In this assessment, you'll implement a basic search engine by defining your own Python classes. A **search engine** is an algorithm that takes a query and retrieves the most relevant documents for that query. In order to identify the most relevant documents, our search engine will use **term frequency–inverse document frequency** ([tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)), an information statistic for determining the relevance of a term to each document from a corpus consisting of many documents.\n",
    "\n",
    "The tf–idf statistic consists of two components: term frequency and inverse document frequency. **Term frequency** computes the number of times that a term appears in a **document** (such as a single Wikipedia page). If we were to use only the term frequency in determining the relevance of a term to each document, then our search result might not be helpful since most documents contain many common words such as \"the\" or \"a\". In order to downweight these common terms, the **document frequency** computes the number of times that a term appears across the **corpus** of all documents. The **tf–idf** statistic takes a term and a document and returns the term frequency divided by the document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5ed02adc-9d59-4e97-bf14-ef02865ea17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: C:\\Users\\Luqman\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "Version 1.0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nb_mypy pytest ipytest\n",
    "%reload_ext nb_mypy\n",
    "%nb_mypy mypy-options --strict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "96ac5cd5-dd9f-4c73-81f2-e2beb91c8952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import re\n",
    "\n",
    "import pytest\n",
    "import ipytest\n",
    "ipytest.autoconfig()\n",
    "\n",
    "\n",
    "def clean(token: str, pattern: re.Pattern[str] = re.compile(r\"\\W+\")) -> str:\n",
    "    \"\"\"\n",
    "    Returns all the characters in the token lowercased and without matches to the given pattern.\n",
    "\n",
    "    >>> clean(\"Hello!\")\n",
    "    'hello'\n",
    "    \"\"\"\n",
    "    return pattern.sub(\"\", token.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f3902f-401a-4637-9b30-5d6fb050f40e",
   "metadata": {},
   "source": [
    "## Task: `Document`\n",
    "\n",
    "Write and test a `Document` class in the code cell below that can be used to represent the text in a web page and includes methods to compute term frequency. (But not document frequency since that would require access to all the documents in the corpus.)\n",
    "\n",
    "The `Document` class should include:\n",
    "\n",
    "1. An initializer `__init__` takes a `str` path and initializes the document data based on the text in the specified file. Assume that the file exists, but that it could be empty. In order to implement `term_frequency` later, we'll need to precompute and save the term frequency for each term in the document in the initializer as a field by constructing a dictionary that maps each `str` term to its `float` term frequency. Term frequency is defined as *the count of the given term* divided by *the total number of words in the text*.\n",
    "> Consider the term frequencies for this document containing 4 total words: \"the cutest cutest dog\".\n",
    ">\n",
    "> - \"the\" appears 1 time out of 4 total words, so its term frequency is 0.25.\n",
    "> - \"cutest\" appears 2 times out of 4 total words, so its term frequency is 0.5.\n",
    "> - \"dog\" appears 1 time out of 4 total words, so its term frequency is 0.25.\n",
    "\n",
    "   When constructing this dictionary, call the `clean` function to lowercasing letters and ignore non-letter characters so that \"corgi\", \"CoRgi\", and \"corgi!!\" are all considered the same string to the search algorithm.\n",
    "\n",
    "1. A method `term_frequency` that takes a given `str` term and returns its term frequency by looking it up in the precomputed dictionary. Remember to normalize the term before looking it up to find the corresponding match. If the term does not occur, return 0.\n",
    "\n",
    "1. A method `get_path` that returns the `str` path of the file that this document represents.\n",
    "\n",
    "1. A method `get_words` that returns a `set` of the unique, cleaned words in this document.\n",
    "\n",
    "1. A method `__repr__` that returns a string representation of this document in the format `Document('{path}')` (with literal single quotes in the output) where `{path}` is the path to the document from the initializer. The `__repr__` method is called when Jupyter Notebook needs to display a `Document` as output, so we should be able to copy the string contents into a new code cell and immediately run it to create a new `Document`.\n",
    "\n",
    "**For each of the 4 methods (excluding the initializer) in the `Document` class, write a testing function that contains at least 3 `pytest`-style assertions based on your own testing corpus**. Documentation strings are optional for testing functions.\n",
    "\n",
    "We've provided some example corpuses in the `doggos` directory and the `small_wiki` directory. For this task, create your own testing corpus by creating a **New Folder** and adding your own text files to it.\n",
    "\n",
    "> Be sure to exhaustively test your `Document` class before moving on: bugs in `Document` will make implementing the following `SearchEngine` task much more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d7e58631-86a0-4dde-a43c-e8bc0cd9e6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                         [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, path: str) -> None:\n",
    "        # initialize fields\n",
    "        self.path = path\n",
    "        self.tf: Dict[str, float] = {}\n",
    "\n",
    "        with open(path) as f:  # parse through the file\n",
    "            lines = f.readlines()\n",
    "            num_tokens = 0     # counter for the tokens\n",
    "            for line in lines:\n",
    "                tokens = line.split()\n",
    "                for token in tokens:\n",
    "                    token = clean(token)\n",
    "                    if token in self.tf.keys():\n",
    "                        self.tf[token] += 1  # update if already in self.tf\n",
    "                    else:\n",
    "                        self.tf[token] = 1   # initialize to 1 if not in self.tf\n",
    "                    num_tokens += 1          # update num_tokens\n",
    "\n",
    "        # iterate through the tokens in self.tf\n",
    "        for token in self.tf.keys():\n",
    "            # reset the value to be the term_frequency\n",
    "            self.tf[token] = float(self.tf[token] / num_tokens)\n",
    "\n",
    "    def term_frequency(self, term: str) -> float:\n",
    "        '''\n",
    "        Takes a given term and finds the term frequency of that term within\n",
    "        the current document. No calculation needed since we preprocessed it\n",
    "        within the initializer\n",
    "\n",
    "        Arguments:\n",
    "            - term: the term to find the term frequency for\n",
    "\n",
    "        Returns:\n",
    "            - the term frequency for the given term. 0 if not in the document\n",
    "        '''\n",
    "        term = clean(term)\n",
    "        if term in self.tf.keys():\n",
    "            return self.tf[term]\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "    def get_path(self) -> str:\n",
    "        '''\n",
    "        Retrieves the path of the current document\n",
    "\n",
    "        Returns:\n",
    "            - the path of the document\n",
    "        '''\n",
    "        return self.path\n",
    "\n",
    "    def get_words(self) -> set[str]:\n",
    "        '''\n",
    "        Finds the set of unique words within the current document\n",
    "\n",
    "        Returns:\n",
    "            - the set of unique, cleaned words in the current document\n",
    "        '''\n",
    "        return set(self.tf.keys())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        '''\n",
    "        Returns a string representation of the current object with its path\n",
    "        '''\n",
    "        return (\"Document('\" + self.path + \"')\")\n",
    "\n",
    "\n",
    "class TestDocument:\n",
    "    euro = Document(\"small_wiki/Euro - Wikipedia.html\")\n",
    "    doc1 = Document(\"doggos/doc1.txt\")\n",
    "    doc2 = Document(\"doggos/my_test.txt\")\n",
    "    ...\n",
    "\n",
    "    def test_term_frequency(self) -> None:\n",
    "        assert self.euro.term_frequency(\"Euro\") == pytest.approx(0.0086340569495348)\n",
    "        assert self.doc1.term_frequency(\"dogs\") == pytest.approx(1 / 5)\n",
    "\n",
    "        # tests the case of a nonexistent word in the document\n",
    "        assert self.doc2.term_frequency(\"nosir\") == 0.0\n",
    "\n",
    "    def test_get_words(self) -> None:\n",
    "        assert set(w for w in self.euro.get_words() if len(w) == 1) == set([\n",
    "            *\"0123456789acefghijklmnopqrstuvxyz\".lower() # All one-letter words in Euro\n",
    "        ])\n",
    "        assert self.doc1.get_words() == set(\"dogs are the greatest pets\".split())\n",
    "\n",
    "        # tests the case of cleaning and uniqueness since doc2 has repeat words\n",
    "        # also tests double whitespace (\"  \")\n",
    "        assert self.doc2.get_words() == set(\"how are you doing\".split())\n",
    "\n",
    "    def test_repr(self) -> None:\n",
    "        # tests the simple functionality of __repr__\n",
    "\n",
    "        assert self.doc1.__repr__() == \"Document('doggos/doc1.txt')\"\n",
    "        assert self.euro.__repr__() == \"Document('small_wiki/Euro - Wikipedia.html')\"\n",
    "\n",
    "    def test_get_path(self) -> None:\n",
    "        # tests the simple functionality of get_path\n",
    "\n",
    "        assert self.doc1.get_path() == \"doggos/doc1.txt\"\n",
    "        assert self.euro.get_path() == \"small_wiki/Euro - Wikipedia.html\"\n",
    "\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "793ae51f-db9c-4859-97a2-a58267ec4627",
   "metadata": {},
   "source": [
    "## Task: `SearchEngine`\n",
    "\n",
    "Write and test a `SearchEngine` class in the code cell below that represents a corpus of `Document` objects and includes methods to compute the tf–idf statistic between a given query and every document in the corpus. The `SearchEngine` begins by constructing an **inverted index** that associates each term in the corpus to the list of `Document` objects that contain the term.\n",
    "\n",
    "To iterate over all the files in a directory, call `os.listdir` to list all the file names and join the directory to the filename with `os.path.join`. The example below will print only the `.txt` files in the `doggos` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9893aeb0-1048-4527-851f-78b77df8c0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doggos\\doc1.txt\n",
      "doggos\\doc2.txt\n",
      "doggos\\doc3.txt\n",
      "doggos\\my_test.txt\n"
     ]
    }
   ],
   "source": [
    "path = \"doggos\"\n",
    "extension = \".txt\"\n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(extension):\n",
    "        print(os.path.join(path, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b7ff12-8c72-49bf-9e5f-45024044340f",
   "metadata": {},
   "source": [
    "The `SearchEngine` class should include:\n",
    "\n",
    "1. An initializer that takes a `str` path to a directory such as `\"small_wiki\"` and a `str` file extension and constructs an inverted index from the files in the specified directory matching the given extension. By default, the extension should be `\".txt\"`. Assume the string represents a valid directory, and that the directory contains only valid files. Do not recreate any behavior that is already done in the `Document` class—call the `get_words()` method! Create at most one `Document` per file.\n",
    "\n",
    "1. A method `_calculate_idf` that takes a `str` term and returns the inverse document frequency of that term. If the term is not in the corpus, return 0. Inverse document frequency is defined by calling `math.log` on the *the total number of documents in the corpus* divided by *the number of documents containing the given term*.\n",
    "\n",
    "1. A method `__repr__` that returns a string representation of this search engine in the format `SearchEngine('{path}')` (with literal single quotes in the output) where `{path}` is the path to the document from the initializer. The `__repr__` method is called when Jupyter Notebook needs to display a `SearchEngine` as output, so we should be able to copy the string contents into a new code cell and immediately run it to create a new `SearchEngine`.\n",
    "\n",
    "1. A method `search` that takes a `str` **query** consisting of one or more terms and returns a `list` of relevant document paths that match at least one of the cleaned terms sorted by descending tf–idf statistic. If there are no matching documents, return an empty list. See also the [Guidance for `SearchEngine.search`](#Guidance-for-SearchEngine.search).\n",
    "\n",
    "**For each of the 3 methods (excluding the initializer) in the `SearchEngine` class, write a testing function that contains at least 3 `pytest`-style assertions based on your own testing corpus**. Documentation strings are optional for testing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ac19da1b-da24-46da-add8-7065e09b5789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                                                          [100%]\u001b[0m\n",
      "\u001b[32m\u001b[32m\u001b[1m3 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%ipytest\n",
    "from typing import Optional, List\n",
    "import pprint\n",
    "\n",
    "class SearchEngine:\n",
    "\n",
    "    def __init__(self, path: str, file_extension: Optional[str] = \".txt\") -> None:\n",
    "        # i understand that i have the Optional thing in the init prototype\n",
    "        # but i get a warning saying that i'm using type None instead of str\n",
    "        # whenever i try to use file_extension\n",
    "        if file_extension is None:\n",
    "            file_extension = \".txt\"\n",
    "\n",
    "        # fields\n",
    "        self.path = path                           # directory path field\n",
    "        self.documents: List[Document] = []        # list of documents in the dir\n",
    "        self.file_extension = file_extension       # the file_extension\n",
    "        self.doc_words: Dict[str, list[str]] = {}  # words to list of docs that contain\n",
    "                                                   # the word\n",
    "\n",
    "        # iterate through the directory specified by the path\n",
    "        for file_name in os.listdir(path):\n",
    "            if file_name.endswith(file_extension): # check file extension\n",
    "                # full_path = os.path.join(path, file_name)\n",
    "                full_path = path + \"/\" + file_name # join the path with the\n",
    "                                                   # file name\n",
    "\n",
    "                if os.path.isfile(full_path):      # is this path a valid one?\n",
    "                    doc = Document(full_path)      # yes, create a new Document obj\n",
    "                    self.documents.append(doc)\n",
    "                    with open(full_path) as f:     # parse through this file\n",
    "                        lines = f.readlines()\n",
    "                        for line in lines:\n",
    "                            tokens = line.split()\n",
    "                            for token in tokens:\n",
    "                                token = clean(token)  # clean the token\n",
    "                                if token in self.doc_words:\n",
    "                                    # only add the path once\n",
    "                                    if full_path not in self.doc_words[token]:\n",
    "                                        self.doc_words[token].append(full_path)\n",
    "                                else:\n",
    "                                    word_list = [full_path]\n",
    "                                    self.doc_words[token] = word_list\n",
    "\n",
    "\n",
    "        # sort the doc_words dictionary by the length of the list of documents\n",
    "        # each word has\n",
    "        sorted_items = sorted(self.doc_words.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "        self.doc_words = dict(sorted_items)\n",
    "\n",
    "    def _calculate_idf(self, term: str) -> float:\n",
    "        '''\n",
    "        Calculates the idf score of a single term in the corpus of documents\n",
    "\n",
    "        Arguments:\n",
    "            - term: the term we want to calculate the idf for\n",
    "\n",
    "        Returns:\n",
    "            - the idf for the term\n",
    "        '''\n",
    "        num = 0\n",
    "        for doc in self.documents:\n",
    "            if term in doc.get_words():\n",
    "                num += 1\n",
    "        if num != 0:\n",
    "            return math.log(len(self.documents) / num)\n",
    "        else:\n",
    "            return 0.0\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        '''\n",
    "        Returns the string output of the SearchEngine object with the path\n",
    "        and the file extension\n",
    "        '''\n",
    "        return \"SearchEngine('\" + self.path + \"', '\" + self.file_extension + \"')\"\n",
    "\n",
    "    def search(self, query: str) -> list[str]:\n",
    "        '''\n",
    "        Takes a string query consisting of one or more terms and finds all the\n",
    "        relevant document paths taht match at least one of the cleaned terms\n",
    "        sorted by descending tf-idf statistic.\n",
    "\n",
    "        Arguments:\n",
    "            - query: the query consisting of one or more terms that we use\n",
    "                     to scower the corpus of documents finding matching paths\n",
    "                     that contain this query\n",
    "\n",
    "        Returns:\n",
    "            - a list of relevant document paths that match at least one of the\n",
    "              cleaned terms sorted by descending tf-idf statistic. If there\n",
    "              are no matching documents, we return an empty list\n",
    "        '''\n",
    "        words = query.split(\" \")  # split the query by white space\n",
    "        doc_tf_idf = {}           # initialize an empty dictionary that will be\n",
    "                                  # used to find the tf_idf for the documents\n",
    "                                  # from the query\n",
    "        for word in words:\n",
    "            word = clean(word)    # clean the word\n",
    "            if word in self.doc_words:\n",
    "                for file in self.doc_words[word]:\n",
    "                    doc = Document(file)\n",
    "\n",
    "                    # calculation of tf_idf\n",
    "                    tf_idf = self._calculate_idf(word) * doc.term_frequency(word)\n",
    "\n",
    "                    if file not in doc_tf_idf:  # add file only once\n",
    "                        doc_tf_idf[file] = tf_idf\n",
    "                    else:\n",
    "                        # this case, we add the tf_idf to the already existing score\n",
    "                        doc_tf_idf[file] += tf_idf\n",
    "\n",
    "\n",
    "        # this line sorts the map into a list (of the keys) by descending order of\n",
    "        # values which in this case, is the tf-idf score for the documents\n",
    "        # matching the query\n",
    "        res = [k for k, v in sorted(doc_tf_idf.items(), key=lambda item:(item[1]), reverse=True)]\n",
    "\n",
    "        if not res:  # if the resulting list is empty, return an empty list\n",
    "            return []\n",
    "        return res\n",
    "\n",
    "class TestSearchEngine:\n",
    "    doggos = SearchEngine(\"doggos\")\n",
    "    small_wiki = SearchEngine(\"small_wiki\", \".html\")\n",
    "    python_files = SearchEngine(\"tests/python\", \".py\")\n",
    "\n",
    "    def test_search(self) -> None:\n",
    "        assert self.doggos.search(\"love dogs\") == [\"doggos/doc3.txt\", \"doggos/doc1.txt\"]\n",
    "        # assert self.small_wiki.search(\"data\")[:10] == [\n",
    "        #     \"small_wiki/Internet privacy - Wikipedia.html\",\n",
    "        #     \"small_wiki/Machine learning - Wikipedia.html\",\n",
    "        #     \"small_wiki/Bloomberg L.P. - Wikipedia.html\",\n",
    "        #     \"small_wiki/Waze - Wikipedia.html\",\n",
    "        #     \"small_wiki/Digital object identifier - Wikipedia.html\",\n",
    "        #     \"small_wiki/Chief financial officer - Wikipedia.html\",\n",
    "        #     \"small_wiki/UNCF - Wikipedia.html\",\n",
    "        #     \"small_wiki/Jackson 5 Christmas Album - Wikipedia.html\",\n",
    "        #     \"small_wiki/KING-FM - Wikipedia.html\",\n",
    "        #     \"small_wiki/The News-Times - Wikipedia.html\",\n",
    "        # ]\n",
    "\n",
    "        # tests searching for a nonexistent word in the corpus of documents\n",
    "        assert self.doggos.search(\"luqman\") == []\n",
    "\n",
    "        # testing search but with a .py extension (different extensions test essentially)\n",
    "        assert self.python_files.search(\"if\") == ['tests/python/test1.py']\n",
    "\n",
    "    def test_repr(self) -> None:\n",
    "        expected_repr = \"SearchEngine('doggos', '.txt')\"\n",
    "        assert repr(self.doggos) == expected_repr\n",
    "\n",
    "    def test_calculate_idf(self) -> None:\n",
    "        # tests basic calculate_idf functionality\n",
    "        assert self.doggos._calculate_idf(\"dogs\") == 0.6931471805599453\n",
    "\n",
    "        # tests 0.0 return of nonexisteent word for calculate_idf\n",
    "        assert self.doggos._calculate_idf(\"world\") == 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63229ee-a94e-4b2d-ba34-4a87cccb2eec",
   "metadata": {},
   "source": [
    "We recommend the following iterative software development approach to implement the `search` method.\n",
    "\n",
    "1. Write code to handle queries that contain only a single term by collecting all the documents that contain the given term, computing the tf–idf statistic for each document, and returning the list of document paths sorted by descending tf–idf statistic.\n",
    "\n",
    "1. Write tests to ensure that your program works on single-term queries.\n",
    "\n",
    "1. Write code to handle queries that contain more than one term by returning all the documents that match any of the terms in the query sorted by descending tf–idf statistic. The tf–idf statistic for a document that matches more than one term is defined as the sum of its constituent single-term tf–idf statistics.\n",
    "\n",
    "1. Write tests to ensure that your program works on multi-term queries.\n",
    "\n",
    "Here's a walkthrough of the `search` function from beginning to end. Say we have a corpus in a directory called `\"doggos\"` containing 3 documents with the following contents:\n",
    "\n",
    "- `doggos/doc1.txt` with the text `Dogs are the greatest pets.`\n",
    "- `doggos/doc2.txt` with the text `Cats seem pretty okay`\n",
    "- `doggos/doc3.txt` with the text `I love dogs!`\n",
    "\n",
    "The initializer should construct the following inverted index.\n",
    "\n",
    "```python\n",
    "{\"dogs\":     [doc1, doc3],\n",
    " \"are\":      [doc1],\n",
    " \"the\":      [doc1],\n",
    " \"greatest\": [doc1],\n",
    " \"pets\":     [doc1],\n",
    " \"cats\":     [doc2],\n",
    " \"seem\":     [doc2],\n",
    " \"pretty\":   [doc2],\n",
    " \"okay\":     [doc2],\n",
    " \"i\":        [doc3],\n",
    " \"love\":     [doc3]}\n",
    "```\n",
    "\n",
    "Searching this corpus for the multi-term query `\"love dogs\"` should return a list `[\"doggos/doc3.txt\", \"doggos/doc1.txt\"]` by:\n",
    "\n",
    "1. Finding all matching documents with at least one query term. `doc3` contains the word `\"love\"` while both `doc1` and `doc3` contain the word `\"dogs\"`. `doc2` does not match any of the query terms.\n",
    "\n",
    "1. Computing the tf–idf statistic for each matching document. For each matching document, the tf–idf statistic for a multi-word query `\"love dogs\"` is the sum of the tf–idf statistics for `\"love\"` and `\"dogs\"` individually.\n",
    "\n",
    "   1. For `doc1`, the sum of 0 + 0.081 = 0.081. The tf–idf statistic for `\"love\"` is 0 because the term is not in `doc1`.\n",
    "\n",
    "   1. For `doc3`, the sum of 0.366 + 0.135 = 0.501.\n",
    "\n",
    "1. Returning the matching document paths sorted by descending tf–idf statistic.\n",
    "\n",
    "After completing your `SearchEngine`, run the following cell to search our small Wikipedia corpus for the query \"data\". Try some other search queries too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0555e6ef-ea88-427b-957d-8cc0bfcc6e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<cell>1: \u001b[1m\u001b[91merror:\u001b[0m Name \u001b[0m\u001b[1m\"SearchEngine\"\u001b[0m is not defined  \u001b[0m\u001b[93m[name-defined]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['small_wiki/Euro - Wikipedia.html']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchEngine(\"small_wiki\", \".html\").search(\"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
